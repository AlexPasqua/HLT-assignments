{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "awful-extra",
   "metadata": {},
   "source": [
    "# Neural Dependency Parsing\n",
    "\n",
    "Derived from code for Stanford CS224N, by:\n",
    "- Sahil Chopra <schopra8@stanford.edu>\n",
    "- Haoshen Hong <haoshen@stanford.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-zambia",
   "metadata": {},
   "source": [
    "In this homework, you’ll be implementing a neural-network based dependency parser with the goal of maximizing performance on the UAS (Unlabeled Attachment Score) metric.\n",
    "\n",
    "A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between head words, and dependent words which modify those heads.\n",
    "There are several types of dependency parsers, including transition-based parsers, graph-based parsers, and feature-based parsers. Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time.\n",
    "The parser maintains a state, which is represented as follows:\n",
    "\n",
    "- A `stack` of words that are currently being processed.\n",
    "- A `buffer` of words yet to be processed.\n",
    "- A `list` of dependencies predicted by the parser.\n",
    "\n",
    "Initially, the stack only contains `ROOT`, the dependencies list is empty, and the buffer contains the list of words of the sentence. At each step, the parser applies a transition to its state until its buffer is empty and the stack size is 1.\n",
    "The following transitions can be applied:\n",
    "\n",
    "- `SHIFT`: removes the first word from the buffer and pushes it onto the stack.\n",
    "- `LEFT-ARC`: marks the second (second most recently added) item on the stack as a dependent of the first item and removes the second item from the stack, adding a first word → second word dependency to the dependency list.\n",
    "- `RIGHT-ARC`: marks the first (most recently added) item on the stack as a dependent of the second item and removes the first item from the stack, adding a second word → first word dependency to the dependency list.\n",
    "\n",
    "On each step, your parser will decide among the three transitions using a neural network classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-teacher",
   "metadata": {},
   "source": [
    "# 1   Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-defendant",
   "metadata": {},
   "source": [
    "## 1.1  Transitions\n",
    "Provide the sequence of Attardi’s non-projective transitions for parsing the following sentence:\n",
    "\n",
    "`The president scheduled a meeting yesterday that nobody attended.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-acting",
   "metadata": {},
   "source": [
    "## 1.2 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-citizenship",
   "metadata": {},
   "source": [
    "*What is the difference in terms of features between neural network dependency parsers (e.g. Chen&Manning 2014, https://cs.stanford.edu/~danqi/papers/emnlp2014.pdf) and non-neural network dependency parsers (e.g. parsers with lots of features like Zhang&Nivre 2011, www.anthology.aclweb.org/P/P11/P11-2033.pdf), in particular in terms of sparsity?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-violin",
   "metadata": {},
   "source": [
    "## 1.3  Ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-astrology",
   "metadata": {},
   "source": [
    "*What is the ambiguity in parsing the following sentence?*<br/>\n",
    "`There are statistics about poverty that no one is willing to accept`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-configuration",
   "metadata": {},
   "source": [
    "## 1.4 Parse Tree"
   ]
  },
  {
   "attachments": {
    "b0612af2-31cc-4a3e-a7e9-d54a7cfa5359.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "super-information",
   "metadata": {},
   "source": [
    "*Mention which errors that make the following an incorrect dependency tree:*\n",
    "\n",
    "![image.png](attachment:b0612af2-31cc-4a3e-a7e9-d54a7cfa5359.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-brief",
   "metadata": {},
   "source": [
    "## Exercise 1.\n",
    "Implement the `__init__` and `step` methods in the `ParseState` class in `parser_state.py`. This implements the transition mechanics your parser will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vocational-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parser_state import ParserState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-soundtrack",
   "metadata": {},
   "source": [
    "We will represent sentences as list of tokens, where tokens are named tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sapphire-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Token = namedtuple('Token', ['id', 'form', 'pos', 'head', 'deprel'], defaults=(0,)*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-rwanda",
   "metadata": {},
   "source": [
    "Example of a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "consolidated-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[Token(id=1, form='The', pos=0, head=0, deprel=0),\n Token(id=2, form='cat', pos=0, head=0, deprel=0)]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Token(1, 'The'), Token(2, 'cat')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-primary",
   "metadata": {},
   "source": [
    "## Test a single parser step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hazardous-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(transition, stack, buf, deps,\n",
    "              ex_stack, ex_buf, ex_deps):\n",
    "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
    "    ps = ParserState([Token(i, f) for i,f in enumerate(stack)],\n",
    "                    [Token(i + len(stack), f) for i,f in enumerate(buf)],\n",
    "                    deps)\n",
    "    \n",
    "    ps.step(ps.tr2id[transition]) # covert action name to it numeric id\n",
    "    stack = [t.form for t in ps.stack] # collect the words\n",
    "    buf = [t.form for t in ps.buffer]\n",
    "    deps = [(a[0].form, a[1].form) for a in sorted(ps.arcs)]\n",
    "    assert stack == ex_stack, \\\n",
    "        f\"{transition} test resulted in stack {stack}, expected {ex_stack}\"\n",
    "    assert buf == ex_buf, \\\n",
    "        f\"{transition} test resulted in buffer {buf}, expected {ex_buf}\"\n",
    "    assert deps == ex_deps, \\\n",
    "        \"{transition} test resulted in dependency list {deps}, expected {ex_deps}\"\n",
    "    print(f\"{transition} test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-somewhere",
   "metadata": {},
   "source": [
    "Perform a few tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "linear-diary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S test passed!\n",
      "LA test passed!\n",
      "RA test passed!\n"
     ]
    }
   ],
   "source": [
    "test_step(\"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
    "          [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [])\n",
    "test_step(\"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
    "          [\"ROOT\", \"cat\"], [\"sat\"], [(\"cat\", \"the\")])\n",
    "test_step(\"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
    "          [\"ROOT\", \"run\"], [], [(\"run\", \"fast\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-occasion",
   "metadata": {},
   "source": [
    "## Test parsing a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adequate-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Token(0, 'ROOT')\n",
    "\n",
    "def test_parse():\n",
    "    \"\"\"Simple tests for the PartialParse.parse function.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    sentence = [Token(i+1, f) for i,f in enumerate([\"parse\", \"this\", \"sentence\"])]\n",
    "    state = ParserState(stack=[ROOT], buffer=sentence)\n",
    "    dependencies = state.parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
    "    dependencies = [(a[0].form, a[1].form) for a in sorted(dependencies)]\n",
    "    expected = [('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this')]\n",
    "    assert dependencies == expected, \\\n",
    "        f\"parse test resulted in dependencies {dependencies}, expected {expected}\"\n",
    "    assert [t.form for t in sentence] == [\"parse\", \"this\", \"sentence\"], \\\n",
    "        f\"parse test failed: the input sentence should not be modified\"\n",
    "    print(\"parse test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "seventh-baltimore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse test passed!\n"
     ]
    }
   ],
   "source": [
    "test_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-affect",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-emerald",
   "metadata": {},
   "source": [
    "We are now going to train a neural network to predict, given the state of the stack, buffer, and dependencies, which transition should be applied next.<br/>\n",
    "First, the model extracts a feature vector representing the current state. We will be using the feature set presented in the  paper by  Chen and Manning (2014), \"A Fast and Accurate Dependency Parser using Neural Networks\", https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf.\n",
    "\n",
    "The method `ParserState.extract_features()` to extract these features is  implemented in `parser_state.py`.\n",
    "These features consist of a triple:\n",
    "- a list of tokens (e.g., the last word in the stack, first word in the buffer, dependent of the second-to-last word in the stack if there is one, etc.).\n",
    "- a list of POS tags for the same tokens\n",
    "- a list of DEPRELs for the same tokens.\n",
    "Each element is represented by an integer ids, and therefore it consists of:\n",
    "\n",
    "$$[ [w_1,w_2,...,w_m], [p_1, p_2,...,p_m], [d_1, d_2,..., d_m] ]$$\n",
    "\n",
    "where $m$ is the number of features and each $0 ≤ w_i < |V|$ is the index of a token in the vocabulary ($|V|$ is the vocabulary size) and similarly for $p_i$ and $d_i$.\n",
    "Then our network looks up an embedding for each word and tags and concatenates them into a single input vector:\n",
    "$$x = [E_{w_1},...,E_{w_m},Ep_{p_1},...,Ep_{p_m},Ed_{d_1},...,Ed_{d_m}] ∈ \\mathbb{R}^{(d+d_p+d_d)m}$$\n",
    "where $E ∈ \\mathbb{R}^{|V|×d}$ is an embedding matrix with each row $E_w$ as the vector for a particular word $w$, and similarly $Ep$ and $Ed$ for tags, with dimesions respectively $d_p$ and $d_d$.<br/>\n",
    "We then compute our prediction as:\n",
    "$$h = ReLU(xW + b_1)$$\n",
    "$$l = hU + b_2$$\n",
    "$$\\hat{y} = softmax(l)$$\n",
    "where $h$ is referred to as the hidden layer, $l$ is referred to as the logits, $\\hat{y}$ is referred to as the predictions, and $ReLU(z) = max(z, 0)$. We will train the model to minimize cross-entropy loss:\n",
    "$$J(θ) = CE(y,\\hat{y}) = \\sum_{i=1}^a{−y_i log \\hat{y}_i}$$\n",
    "where $a$ is the number of possible parser actions.\n",
    "To compute the loss for the training set, we average this $J(θ)$ across all training examples.\n",
    "We will use UAS score as our evaluation metric. UAS refers to Unlabeled Attachment Score, which is computed as the ratio between number of correctly predicted dependencies and the number of total dependencies irrespective of the relations.\n",
    "\n",
    "In `model.py` you will find skeleton code to implement this simple neural network using Keras. Complete the `__init__` methods to implement the model.\n",
    "\n",
    "Then complete the train for epoch and train functions. Finally execute python `run.py` to train your model and compute predictions on test data from Penn Treebank (annotated with Universal Dependencies), available in files `data/traing.gold.conll`, `data/dev.gold.conll` and `data/test.gold.conll`.\n",
    "\n",
    "##Note:##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-iceland",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "varying-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus import read_conll\n",
    "\n",
    "train_file = 'data/train.gold.conll'\n",
    "dev_file = 'data/dev.gold.conll'\n",
    "test_file = 'data/test.gold.conll'\n",
    "\n",
    "max_sent = 500 # limit sentences during development\n",
    "\n",
    "train_sents = read_conll(train_file, max_sent=max_sent)\n",
    "dev_sents = read_conll(dev_file, max_sent=max_sent//2)\n",
    "test_sents = read_conll(test_file, max_sent=max_sent//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-spelling",
   "metadata": {},
   "source": [
    "## Create the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "blond-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parser import Parser\n",
    "\n",
    "parser = Parser(train_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-fishing",
   "metadata": {},
   "source": [
    "## Convert to numeric vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accepting-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = parser.vectorize(train_sents)\n",
    "dev_vectors = parser.vectorize(dev_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-threat",
   "metadata": {},
   "source": [
    "## Build the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "empty-theory",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [05:42<00:00,  1.46it/s]\n",
      "100%|██████████| 250/250 [04:30<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = parser.create_features(train_vectors)\n",
    "dev_x, dev_y = parser.create_features(dev_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-hardware",
   "metadata": {},
   "source": [
    "Show sample of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hairy-toyota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(([2, 0, 3228, 1469, 310, 3195, 2, 235, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "  [2, 0, 8, 12, 34, 21, 2, 38, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_x[3], dev_y[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-korea",
   "metadata": {},
   "source": [
    "## Build the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "latest-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset\n",
    "\n",
    "ds_train = Dataset.from_tensor_slices((train_x, train_y)).shuffle(1000).batch(32)\n",
    "ds_dev = Dataset.from_tensor_slices((dev_x, dev_y)).shuffle(1000).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-rental",
   "metadata": {},
   "source": [
    "## Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mobile-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Glove\n",
    "\n",
    "emb_path = 'data/en-cw.txt'\n",
    "\n",
    "glove = Glove(glove_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-religious",
   "metadata": {},
   "source": [
    "## Prepare embedding matrix\n",
    "Trimmed to the parser vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sophisticated-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(parser.tok2id)\n",
    "embedding_dim = len(next(iter(glove.wv.values())))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Fill the matrix with Glove embeddings\n",
    "embedding_matrix = np.random.uniform(-1, 1, (num_tokens, embedding_dim))\n",
    "for word, i in parser.tok2id.items():\n",
    "    embedding_vector = glove.wv.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be random.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-support",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ParserModel\n",
    "\n",
    "n_features = len(train_x[0][0])\n",
    "n_pos = len(parser.pos2id)\n",
    "n_tags = len(parser.dep2id)\n",
    "tag_size = 20 # size of embeddings for POS and DEPRELs \n",
    "n_actions = n_tags * 2 + 1 # L-d + R-d + 1\n",
    "hidden_size = 200\n",
    "\n",
    "model = ParserModel(embeddings=embedding_matrix, n_features=n_features,\n",
    "                    n_pos=n_pos, n_tags=n_tags, tag_size=tag_size,\n",
    "                    n_actions=n_actions, hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-patrick",
   "metadata": {},
   "source": [
    "Show the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-vanilla",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-elite",
   "metadata": {},
   "source": [
    "Choose an optimizer: `SparseCategoricalCrossentropy` expects numerical categories.\n",
    "\n",
    "Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result.\n",
    "\n",
    "Compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model.compile(\n",
    "    # Optimizer\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(name='train_loss'),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name='train UAS')],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-wheel",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "history = model.fit(ds_train, epochs=EPOCHS,\n",
    "                    validation_data=ds_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-prince",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "UAS, LAS = parser.parse(test_sents, model)\n",
    "print(f'UAS: {UAS*100:.2f}, LAS: {LAS*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-birthday",
   "metadata": {},
   "source": [
    "If you have time: train the parser on the whole training set with more EPOCS and report the UAS/LAS taht you achiecve on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-maine",
   "metadata": {},
   "source": [
    "## Exercise 3 (optional)\n",
    "\n",
    "Modify the `Parser.parse()` method to print the parsed sentences in CoNLL-U format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-shelf",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Let's explore the ability of the parser to handle common mistakes.\n",
    "\n",
    "Consider the following kind of errors:\n",
    "\n",
    "1. **Prepositional Phrase Attachement Error.**\n",
    "This occurs when a prepositional phrase is attached to wrong head word. \n",
    "1. **Verb Phrase Attachement Error.**\n",
    "This occurrs when a verb phrase is attached to the wrong head word.\n",
    "1. **Modifier Attachemnt Error.**\n",
    "This occurrs when a modifier (adjective or adverb) is attached to the wrong head word.\n",
    "1. Coordination Attachment Error:\n",
    "This occurrs when a conjuct is attached to the wrong head wor.\n",
    "\n",
    "Check whether the parser does any of these mistakes on these sentences and classify them according to the 4 types above:\n",
    "\n",
    "- `Moscow sent troops to Afghaninstan`\n",
    "- `I disembarked and was heading to a wedding fearing for my death`\n",
    "- `It makes me want to rush out and rescue people from dilemmas of their own making.`\n",
    "- `Brian has been one of the most crucial elements to the success of Mozilla.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-mystery",
   "metadata": {},
   "source": [
    "# Submission instructions\n",
    "\n",
    "1. Make a zip with your code and notebook (without the data folder), i.e.\n",
    "\n",
    "    zip &lt;YourName&gt;.zip *.ipynb *.py\n",
    "    \n",
    "1. Submit though Moodle on https://elearning.di.unipi.it/mod/assign/view.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-planner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}